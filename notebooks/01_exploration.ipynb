{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Download sentence tokenizer\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c624db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegasus model for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff85e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for computation (GPU preferred)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "# Load PEGASUS model and tokenizer\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from disk\n",
    "dataset_samsum = load_from_disk('samsum_dataset')\n",
    "dataset_samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0691cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729427f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    \"\"\"\n",
    "    Tokenizes a batch of dialogueâ€“summary pairs for PEGASUS.\n",
    "    Converts raw text into input IDs and labels that the model can process.\n",
    "    \"\"\"\n",
    "    # Tokenize dialogues (model inputs)\n",
    "    input_encodings = tokenizer(\n",
    "        batch['dialogue'],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Tokenize summaries (model targets)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(\n",
    "            batch['summary'],\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    # Return tokenized input and labels\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "# Apply preprocessing to all splits\n",
    "tokenized_dataset = dataset_samsum.map(\n",
    "    preprocess_batch, \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Check a sample to confirm encoding\n",
    "tokenized_dataset[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e936ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize summarization pipeline with our PEGASUS model\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model_pegasus,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test summarization on one dialogue\n",
    "sample_text = dataset_samsum[\"test\"][1][\"dialogue\"]\n",
    "print(\"Original Dialogue:\\n\", sample_text)\n",
    "summary = summarizer(sample_text, min_length=30, max_length=100)[0]['summary_text']\n",
    "print(\"\\nGenerated Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "# Compute ROUGE scores on a few samples\n",
    "for idx in range(3):\n",
    "    dialogue = dataset_samsum[\"test\"][idx][\"dialogue\"]\n",
    "    reference = dataset_samsum[\"test\"][idx][\"summary\"]\n",
    "    prediction = summarizer(dialogue, min_length=30, max_length=100)[0][\"summary_text\"]\n",
    "    \n",
    "    print(f\"\\nDialogue {idx+1}:\")\n",
    "    print(\"Generated Summary:\", prediction)\n",
    "    print(\"Reference Summary:\", reference)\n",
    "\n",
    "    scores = rouge.compute(predictions=[prediction], references=[reference])\n",
    "    print(\"ROUGE Scores:\", scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
